{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f88aade6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nvme2/gnndataset/venv_v1/lib64/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d3c07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4b7846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGL_tiny(DGLDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='IGL_tiny')\n",
    "\n",
    "    def process(self):\n",
    "        edges_data = torch.from_numpy(np.load(\"../IllinoisGraphDataset_v1/IGB_small/paper_edges.npy\"))\n",
    "        node_features = torch.from_numpy(np.load(\"../IllinoisGraphDataset_v1/IGB_small/paper_emb.npy\"))\n",
    "        node_labels = torch.from_numpy(np.load(\"../IllinoisGraphDataset_v1/IGB_small/paper_label.npy\"))\n",
    "        \n",
    "        self.graph = dgl.graph((edges_data[0, :], edges_data[1, :]), num_nodes=node_features.shape[0])\n",
    "        self.graph.ndata['feat'] = node_features\n",
    "        self.graph.ndata['label'] = node_labels\n",
    "        \n",
    "        n_nodes = node_features.shape[0]\n",
    "\n",
    "        n_train = int(n_nodes * 0.6)\n",
    "        n_val = int(n_nodes * 0.2)\n",
    "        \n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        \n",
    "        train_mask[:n_train] = True\n",
    "        val_mask[n_train:n_train + n_val] = True\n",
    "        test_mask[n_train + n_val:] = True\n",
    "        \n",
    "        self.graph.ndata['train_mask'] = train_mask\n",
    "        self.graph.ndata['val_mask'] = val_mask\n",
    "        self.graph.ndata['test_mask'] = test_mask\n",
    "        \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.graph\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7861c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = IGL_tiny()\n",
    "g = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c345716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=168319, num_edges=395888,\n",
       "      ndata_schemes={'feat': Scheme(shape=(384,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2b87a0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "523af948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.nn.pytorch import GATConv, GraphConv, SAGEConv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7d1fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 in_dim,\n",
    "                 num_hidden,\n",
    "                 num_classes,\n",
    "                 heads,\n",
    "                 activation,\n",
    "                 feat_drop,\n",
    "                 attn_drop,\n",
    "                 negative_slope,\n",
    "                 residual):\n",
    "        super(GAT, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "        # input projection (no residual)\n",
    "        self.gat_layers.append(GATConv(\n",
    "            in_dim, num_hidden, heads[0],\n",
    "            feat_drop, attn_drop, negative_slope, False, self.activation))\n",
    "        # hidden layers\n",
    "        for l in range(1, num_layers):\n",
    "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
    "            self.gat_layers.append(GATConv(\n",
    "                num_hidden * heads[l-1], num_hidden, heads[l],\n",
    "                feat_drop, attn_drop, negative_slope, residual, self.activation))\n",
    "        # output projection\n",
    "        self.gat_layers.append(GATConv(\n",
    "            num_hidden * heads[-2], num_classes, heads[-1],\n",
    "            feat_drop, attn_drop, negative_slope, residual, None))\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = inputs\n",
    "        for l in range(self.num_layers):\n",
    "            h = self.gat_layers[l](g, h).flatten(1)\n",
    "        # output projection\n",
    "        logits = self.gat_layers[-1](g, h).mean(1)\n",
    "        return logits\n",
    "\n",
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels) * 100\n",
    "\n",
    "def track_acc(g):\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "\n",
    "    in_feats = features.shape[1]\n",
    "    n_classes = 19\n",
    "\n",
    "    g = dgl.remove_self_loop(g)\n",
    "    g = dgl.add_self_loop(g)\n",
    "\n",
    "    # create model\n",
    "    model = GAT(1, in_feats, 8, n_classes, [8, 1], F.elu,\n",
    "                0.6, 0.6, 0.2, False)\n",
    "    loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=1e-2,\n",
    "                                 weight_decay=5e-4)\n",
    "    for epoch in range(200):\n",
    "        logits = model(g, features)\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    acc = evaluate(model, g, features, labels, test_mask)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75753956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.75672062973415"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_acc(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac9caa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(GraphConv(in_feats, n_hidden, activation=activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation))\n",
    "        # output layer\n",
    "        self.layers.append(GraphConv(n_hidden, n_classes))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(g, h)\n",
    "        return h\n",
    "\n",
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels) * 100\n",
    "\n",
    "def track_acc(g):\n",
    "    \n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "\n",
    "    in_feats = features.shape[1]\n",
    "    n_classes = 19\n",
    "\n",
    "    g = dgl.remove_self_loop(g)\n",
    "    g = dgl.add_self_loop(g)\n",
    "\n",
    "    # normalization\n",
    "    degs = g.in_degrees().float()\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "    # create GCN model\n",
    "    model = GCN(in_feats, 16, n_classes, 1, F.relu, 0.5)\n",
    "    loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=1e-2,\n",
    "                                 weight_decay=5e-4)\n",
    "    for epoch in range(200):\n",
    "        logits = model(g, features)\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    acc = evaluate(model, g, features, labels, test_mask)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d92030f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.93702658547453"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_acc(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44b5aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 aggregator_type):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "\n",
    "        # input layer\n",
    "        self.layers.append(SAGEConv(in_feats, n_hidden, aggregator_type))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(SAGEConv(n_hidden, n_hidden, aggregator_type))\n",
    "        # output layer\n",
    "        self.layers.append(SAGEConv(n_hidden, n_classes, aggregator_type)) # activation None\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        h = self.dropout(inputs)\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            h = layer(graph, h)\n",
    "            if l != len(self.layers) - 1:\n",
    "                h = self.activation(h)\n",
    "                h = self.dropout(h)\n",
    "        return h\n",
    "\n",
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels) * 100\n",
    "\n",
    "\n",
    "def track_acc(g):\n",
    "\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "\n",
    "    in_feats = features.shape[1]\n",
    "    n_classes = 19\n",
    "\n",
    "    g = dgl.remove_self_loop(g)\n",
    "    g = dgl.add_self_loop(g)\n",
    "\n",
    "    # create model\n",
    "    model = GraphSAGE(in_feats, 16, n_classes, 1, F.relu, 0.5, 'gcn')\n",
    "    loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=1e-2,\n",
    "                                 weight_decay=5e-4)\n",
    "    for epoch in range(200):\n",
    "        logits = model(g, features)\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = evaluate(model, g, features, labels, test_mask)\n",
    "        print(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c4994ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.832763998217732\n",
      "32.187732065943855\n",
      "39.78315758205852\n",
      "43.80216842417941\n",
      "46.34189811376801\n",
      "47.34887865735928\n",
      "47.42016931531264\n",
      "47.437991979800984\n",
      "47.41422842714986\n",
      "47.44393286796376\n",
      "47.52413485816129\n",
      "47.74988860834695\n",
      "48.141987227090446\n",
      "48.71528293479875\n",
      "49.59750482697164\n",
      "50.640130699539576\n",
      "51.69463834843309\n",
      "52.81449576711719\n",
      "53.79771275805732\n",
      "54.6205257686024\n",
      "55.17896925590375\n",
      "55.67800386157731\n",
      "56.06713203623942\n",
      "56.527550868854895\n",
      "56.901826823110056\n",
      "57.216693895737414\n",
      "57.72761027773652\n",
      "58.03653646220109\n",
      "58.39298975196792\n",
      "58.66330016337442\n",
      "58.91875835437398\n",
      "59.09995544333878\n",
      "59.28709342046636\n",
      "59.45937917718699\n",
      "59.58116738452398\n",
      "59.69404425961681\n",
      "59.86930046041883\n",
      "60.01188177632556\n",
      "60.17228575672063\n",
      "60.37427595425516\n",
      "60.6030001485222\n",
      "60.95054210604486\n",
      "61.2980840635675\n",
      "61.73770978761325\n",
      "62.10604485370563\n",
      "62.42982325857716\n",
      "62.77736521609981\n",
      "63.1427298381108\n",
      "63.42492202584287\n",
      "63.73384821030744\n",
      "64.00415862171394\n",
      "64.35467102331799\n",
      "64.65765631961979\n",
      "64.91311451061934\n",
      "65.19233625427002\n",
      "65.37650378731621\n",
      "65.59631664933909\n",
      "65.72404574483886\n",
      "65.83989306401307\n",
      "66.02703104114065\n",
      "66.24387345908214\n",
      "66.39536610723303\n",
      "66.51715431457002\n",
      "66.64191296598841\n",
      "66.78449428189515\n",
      "66.91222337739492\n",
      "67.0102480320808\n",
      "67.08747957819693\n",
      "67.20035645328977\n",
      "67.31620377246398\n",
      "67.36373087776623\n",
      "67.4320510916382\n",
      "67.5211644140799\n",
      "67.58651418387049\n",
      "67.65483439774246\n",
      "67.71424327937027\n",
      "67.80038615773059\n",
      "67.85088370711422\n",
      "67.93405614139314\n",
      "68.01128768750928\n",
      "68.04693301648597\n",
      "68.0974305658696\n",
      "68.11822367443932\n",
      "68.13604633892767\n",
      "68.16278033566017\n",
      "68.17466211198574\n",
      "68.20139610871824\n",
      "68.23110054953216\n",
      "68.25189365810188\n",
      "68.26080499034606\n",
      "68.30536165156691\n",
      "68.31724342789246\n",
      "68.32912520421803\n",
      "68.3647705331947\n",
      "68.37962275360167\n",
      "68.37962275360167\n",
      "68.4004158621714\n",
      "68.41229763849695\n",
      "68.43606119114808\n",
      "68.45388385563642\n",
      "68.47170652012476\n",
      "68.49249962869449\n",
      "68.52517451358977\n",
      "68.53705628991534\n",
      "68.55190851032229\n",
      "68.55487895440369\n",
      "68.58755383929898\n",
      "68.59646517154314\n",
      "68.60537650378731\n",
      "68.59349472746176\n",
      "68.59646517154314\n",
      "68.62022872419426\n",
      "68.61725828011288\n",
      "68.63805138868261\n",
      "68.64993316500816\n",
      "68.641021832764\n",
      "68.63805138868261\n",
      "68.65884449725233\n",
      "68.65587405317095\n",
      "68.641021832764\n",
      "68.66478538541512\n",
      "68.6736967176593\n",
      "68.67963760582208\n",
      "68.68854893806625\n",
      "68.70637160255458\n",
      "68.72419426704293\n",
      "68.7301351552057\n",
      "68.74201693153127\n",
      "68.75686915193822\n",
      "68.76578048418239\n",
      "68.76875092826378\n",
      "68.77172137234517\n",
      "68.7925144809149\n",
      "68.80142581315907\n",
      "68.81033714540324\n",
      "68.82815980989157\n",
      "68.83410069805436\n",
      "68.8519233625427\n",
      "68.86677558294964\n",
      "68.89350957968216\n",
      "68.87271647111243\n",
      "68.88756869151939\n",
      "68.9143026882519\n",
      "68.91727313233328\n",
      "68.90242091192633\n",
      "68.91727313233328\n",
      "68.92321402049606\n",
      "68.93212535274024\n",
      "68.93212535274024\n",
      "68.94994801722858\n",
      "68.96777068171691\n",
      "68.97074112579831\n",
      "68.98856379028665\n",
      "68.99153423436803\n",
      "69.0034160106936\n",
      "69.01529778701916\n",
      "69.01826823110055\n",
      "69.02717956334472\n",
      "69.04203178375167\n",
      "69.05391356007723\n",
      "69.07767711272835\n",
      "69.08064755680974\n",
      "69.08955888905392\n",
      "69.08658844497252\n",
      "69.09847022129809\n",
      "69.09847022129809\n",
      "69.10144066537947\n",
      "69.11629288578642\n",
      "69.11332244170504\n",
      "69.1222337739492\n",
      "69.13114510619339\n",
      "69.12817466211199\n",
      "69.13114510619339\n",
      "69.13708599435616\n",
      "69.16084954700727\n",
      "69.16976087925146\n",
      "69.16976087925146\n",
      "69.15787910292589\n",
      "69.17273132333284\n",
      "69.18461309965839\n",
      "69.19649487598396\n",
      "69.19649487598396\n",
      "69.19352443190256\n",
      "69.18758354373979\n",
      "69.20540620822813\n",
      "69.20540620822813\n",
      "69.20243576414674\n",
      "69.20837665230952\n",
      "69.21728798455369\n",
      "69.22916976087924\n",
      "69.22619931679786\n",
      "69.23511064904203\n",
      "69.22916976087924\n",
      "69.24105153720481\n",
      "69.24996286944898\n",
      "69.28560819842566\n",
      "69.28263775434428\n",
      "69.3064013069954\n",
      "69.30343086291401\n",
      "69.31531263923956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.31531263923956"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_acc(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94b731f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
